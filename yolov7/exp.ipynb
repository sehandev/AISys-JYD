{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.general import coco80_to_coco91_class, check_dataset, check_file, check_img_size, check_requirements, \\\n",
    "    box_iou, non_max_suppression, scale_coords, xyxy2xywh, xywh2xyxy, set_logging, increment_path, colorstr\n",
    "from utils.metrics import ap_per_class, ConfusionMatrix\n",
    "from utils.plots import plot_images, output_to_target, plot_study_txt\n",
    "from utils.torch_utils import select_device, time_synchronized, TracedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(data,\n",
    "         weights=None,\n",
    "         batch_size=32,\n",
    "         imgsz=640,\n",
    "         conf_thres=0.001,\n",
    "         iou_thres=0.6,  # for NMS\n",
    "         save_json=False,\n",
    "         single_cls=False,\n",
    "         augment=False,\n",
    "         verbose=False,\n",
    "         model=None,\n",
    "         dataloader=None,\n",
    "         save_dir=Path(''),  # for saving images\n",
    "         save_txt=False,  # for auto-labelling\n",
    "         save_hybrid=False,  # for hybrid auto-labelling\n",
    "         save_conf=False,  # save auto-label confidences\n",
    "         plots=True,\n",
    "         wandb_logger=None,\n",
    "         compute_loss=None,\n",
    "         half_precision=True,\n",
    "         trace=False,\n",
    "         is_coco=False,\n",
    "         v5_metric=False):\n",
    "    # Initialize/load model and set device\n",
    "    training = model is not None\n",
    "    if training:  # called by train.py\n",
    "        device = next(model.parameters()).device  # get model device\n",
    "\n",
    "    else:  # called directly\n",
    "        set_logging()\n",
    "        device = select_device(opt.device, batch_size=batch_size)\n",
    "\n",
    "        # Directories\n",
    "        save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n",
    "        (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "        # Load model\n",
    "        model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "        gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "        imgsz = check_img_size(imgsz, s=gs)  # check img_size\n",
    "        \n",
    "        if trace:\n",
    "            model = TracedModel(model, device, imgsz)\n",
    "\n",
    "    # Half\n",
    "    half = device.type != 'cpu' and half_precision  # half precision only supported on CUDA\n",
    "    if half:\n",
    "        model.half()\n",
    "\n",
    "    # Configure\n",
    "    model.eval()\n",
    "    if isinstance(data, str):\n",
    "        is_coco = data.endswith('coco.yaml')\n",
    "        with open(data) as f:\n",
    "            data = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    check_dataset(data)  # check\n",
    "    nc = 1 if single_cls else int(data['nc'])  # number of classes\n",
    "    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
    "    niou = iouv.numel()\n",
    "\n",
    "    # Logging\n",
    "    log_imgs = 0\n",
    "    if wandb_logger and wandb_logger.wandb:\n",
    "        log_imgs = min(wandb_logger.log_imgs, 100)\n",
    "    # Dataloader\n",
    "    if not training:\n",
    "        if device.type != 'cpu':\n",
    "            model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "        task = opt.task if opt.task in ('train', 'val', 'test') else 'val'  # path to train/val/test images\n",
    "        dataloader = create_dataloader(data[task], imgsz, batch_size, gs, opt, pad=0.5, rect=True,\n",
    "                                       prefix=colorstr(f'{task}: '))[0]\n",
    "\n",
    "    if v5_metric:\n",
    "        print(\"Testing with YOLOv5 AP metric...\")\n",
    "    \n",
    "    seen = 0\n",
    "    confusion_matrix = ConfusionMatrix(nc=nc)\n",
    "    names = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\n",
    "    coco91class = coco80_to_coco91_class()\n",
    "    s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n",
    "    p, r, f1, mp, mr, map50, map, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n",
    "    loss = torch.zeros(3, device=device)\n",
    "    jdict, stats, ap, ap_class, wandb_images = [], [], [], [], []\n",
    "    for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n",
    "        img = img.to(device, non_blocking=True)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        targets = targets.to(device)\n",
    "        nb, _, height, width = img.shape  # batch size, channels, height, width\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Run model\n",
    "            t = time_synchronized()\n",
    "            out, train_out = model(img, augment=augment)  # inference and training outputs\n",
    "            t0 += time_synchronized() - t\n",
    "\n",
    "            # Compute loss\n",
    "            if compute_loss:\n",
    "                loss += compute_loss([x.float() for x in train_out], targets)[1][:3]  # box, obj, cls\n",
    "\n",
    "            # Run NMS\n",
    "            targets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)  # to pixels\n",
    "            lb = [targets[targets[:, 0] == i, 1:] for i in range(nb)] if save_hybrid else []  # for autolabelling\n",
    "            t = time_synchronized()\n",
    "            out = non_max_suppression(out, conf_thres=conf_thres, iou_thres=iou_thres, labels=lb, multi_label=True)\n",
    "            t1 += time_synchronized() - t\n",
    "\n",
    "        # Statistics per image\n",
    "        for si, pred in enumerate(out):\n",
    "            labels = targets[targets[:, 0] == si, 1:]\n",
    "            nl = len(labels)\n",
    "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "            path = Path(paths[si])\n",
    "            seen += 1\n",
    "\n",
    "            if len(pred) == 0:\n",
    "                if nl:\n",
    "                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
    "                continue\n",
    "\n",
    "            # Predictions\n",
    "            predn = pred.clone()\n",
    "            scale_coords(img[si].shape[1:], predn[:, :4], shapes[si][0], shapes[si][1])  # native-space pred\n",
    "\n",
    "            # Append to text file\n",
    "            if save_txt:\n",
    "                gn = torch.tensor(shapes[si][0])[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "                for *xyxy, conf, cls in predn.tolist():\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
    "                    with open(save_dir / 'labels' / (path.stem + '.txt'), 'a') as f:\n",
    "                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "            # W&B logging - Media Panel Plots\n",
    "            if len(wandb_images) < log_imgs and wandb_logger.current_epoch > 0:  # Check for test operation\n",
    "                if wandb_logger.current_epoch % wandb_logger.bbox_interval == 0:\n",
    "                    box_data = [{\"position\": {\"minX\": xyxy[0], \"minY\": xyxy[1], \"maxX\": xyxy[2], \"maxY\": xyxy[3]},\n",
    "                                 \"class_id\": int(cls),\n",
    "                                 \"box_caption\": \"%s %.3f\" % (names[cls], conf),\n",
    "                                 \"scores\": {\"class_score\": conf},\n",
    "                                 \"domain\": \"pixel\"} for *xyxy, conf, cls in pred.tolist()]\n",
    "                    boxes = {\"predictions\": {\"box_data\": box_data, \"class_labels\": names}}  # inference-space\n",
    "                    wandb_images.append(wandb_logger.wandb.Image(img[si], boxes=boxes, caption=path.name))\n",
    "            wandb_logger.log_training_progress(predn, path, names) if wandb_logger and wandb_logger.wandb_run else None\n",
    "\n",
    "            # Append to pycocotools JSON dictionary\n",
    "            if save_json:\n",
    "                # [{\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}, ...\n",
    "                image_id = int(path.stem) if path.stem.isnumeric() else path.stem\n",
    "                box = xyxy2xywh(predn[:, :4])  # xywh\n",
    "                box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n",
    "                for p, b in zip(pred.tolist(), box.tolist()):\n",
    "                    jdict.append({'image_id': image_id,\n",
    "                                  'category_id': coco91class[int(p[5])] if is_coco else int(p[5]),\n",
    "                                  'bbox': [round(x, 3) for x in b],\n",
    "                                  'score': round(p[4], 5)})\n",
    "\n",
    "            # Assign all predictions as incorrect\n",
    "            correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n",
    "            if nl:\n",
    "                detected = []  # target indices\n",
    "                tcls_tensor = labels[:, 0]\n",
    "\n",
    "                # target boxes\n",
    "                tbox = xywh2xyxy(labels[:, 1:5])\n",
    "                scale_coords(img[si].shape[1:], tbox, shapes[si][0], shapes[si][1])  # native-space labels\n",
    "                if plots:\n",
    "                    confusion_matrix.process_batch(predn, torch.cat((labels[:, 0:1], tbox), 1))\n",
    "\n",
    "                # Per target class\n",
    "                for cls in torch.unique(tcls_tensor):\n",
    "                    ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)  # prediction indices\n",
    "                    pi = (cls == pred[:, 5]).nonzero(as_tuple=False).view(-1)  # target indices\n",
    "\n",
    "                    # Search for detections\n",
    "                    if pi.shape[0]:\n",
    "                        # Prediction to target ious\n",
    "                        ious, i = box_iou(predn[pi, :4], tbox[ti]).max(1)  # best ious, indices\n",
    "\n",
    "                        # Append detections\n",
    "                        detected_set = set()\n",
    "                        for j in (ious > iouv[0]).nonzero(as_tuple=False):\n",
    "                            d = ti[i[j]]  # detected target\n",
    "                            if d.item() not in detected_set:\n",
    "                                detected_set.add(d.item())\n",
    "                                detected.append(d)\n",
    "                                correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n",
    "                                if len(detected) == nl:  # all targets already located in image\n",
    "                                    break\n",
    "\n",
    "            # Append statistics (correct, conf, pcls, tcls)\n",
    "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
    "\n",
    "        # Plot images\n",
    "        if plots and batch_i < 3:\n",
    "            f = save_dir / f'test_batch{batch_i}_labels.jpg'  # labels\n",
    "            Thread(target=plot_images, args=(img, targets, paths, f, names), daemon=True).start()\n",
    "            f = save_dir / f'test_batch{batch_i}_pred.jpg'  # predictions\n",
    "            Thread(target=plot_images, args=(img, output_to_target(out), paths, f, names), daemon=True).start()\n",
    "\n",
    "    # Compute statistics\n",
    "    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
    "    if len(stats) and stats[0].any():\n",
    "        p, r, ap, f1, ap_class = ap_per_class(*stats, plot=plots, v5_metric=v5_metric, save_dir=save_dir, names=names)\n",
    "        ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
    "    else:\n",
    "        nt = torch.zeros(1)\n",
    "\n",
    "    # Print results\n",
    "    pf = '%20s' + '%12i' * 2 + '%12.3g' * 4  # print format\n",
    "    print(pf % ('all', seen, nt.sum(), mp, mr, map50, map))\n",
    "\n",
    "    # Print results per class\n",
    "    if (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):\n",
    "        for i, c in enumerate(ap_class):\n",
    "            print(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n",
    "\n",
    "    # Print speeds\n",
    "    t = tuple(x / seen * 1E3 for x in (t0, t1, t0 + t1)) + (imgsz, imgsz, batch_size)  # tuple\n",
    "    if not training:\n",
    "        print('Speed: %.1f/%.1f/%.1f ms inference/NMS/total per %gx%g image at batch-size %g' % t)\n",
    "\n",
    "    # Plots\n",
    "    if plots:\n",
    "        confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))\n",
    "        if wandb_logger and wandb_logger.wandb:\n",
    "            val_batches = [wandb_logger.wandb.Image(str(f), caption=f.name) for f in sorted(save_dir.glob('test*.jpg'))]\n",
    "            wandb_logger.log({\"Validation\": val_batches})\n",
    "    if wandb_images:\n",
    "        wandb_logger.log({\"Bounding Box Debugger/Images\": wandb_images})\n",
    "\n",
    "    # Save JSON\n",
    "    if save_json and len(jdict):\n",
    "        w = Path(weights[0] if isinstance(weights, list) else weights).stem if weights is not None else ''  # weights\n",
    "        anno_json = './coco/annotations/instances_val2017.json'  # annotations json\n",
    "        pred_json = str(save_dir / f\"{w}_predictions.json\")  # predictions json\n",
    "        print('\\nEvaluating pycocotools mAP... saving %s...' % pred_json)\n",
    "        with open(pred_json, 'w') as f:\n",
    "            json.dump(jdict, f)\n",
    "\n",
    "        try:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\n",
    "            from pycocotools.coco import COCO\n",
    "            from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "            anno = COCO(anno_json)  # init annotations api\n",
    "            pred = anno.loadRes(pred_json)  # init predictions api\n",
    "            eval = COCOeval(anno, pred, 'bbox')\n",
    "            if is_coco:\n",
    "                eval.params.imgIds = [int(Path(x).stem) for x in dataloader.dataset.img_files]  # image IDs to evaluate\n",
    "            eval.evaluate()\n",
    "            eval.accumulate()\n",
    "            eval.summarize()\n",
    "            map, map50 = eval.stats[:2]  # update results (mAP@0.5:0.95, mAP@0.5)\n",
    "        except Exception as e:\n",
    "            print(f'pycocotools unable to run: {e}')\n",
    "\n",
    "    # Return results\n",
    "    model.float()  # for training\n",
    "    if not training:\n",
    "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
    "        print(f\"Results saved to {save_dir}{s}\")\n",
    "    maps = np.zeros(nc) + map\n",
    "    for i, c in enumerate(ap_class):\n",
    "        maps[c] = ap[i]\n",
    "    return (mp, mr, map50, map, *(loss.cpu() / len(dataloader)).tolist()), maps, t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights='yolo7.pt', data='data/coco.yaml', batch_size=32, img_size=640, conf_thres=0.001, iou_thres=0.65, task='val', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project='runs/test', name='yolov7_640_val', exist_ok=False, no_trace=False, v5_metric=False, img=640, batch=32, conf=0.001, iou=0.65)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if __name__ == '__main__':\n",
    "parser = argparse.ArgumentParser(prog='test.py')\n",
    "parser.add_argument('--weights', nargs='+', type=str, default='yolov7.pt', help='model.pt path(s)')\n",
    "parser.add_argument('--data', type=str, default='data/coco.yaml', help='*.data path')\n",
    "parser.add_argument('--batch-size', type=int, default=32, help='size of each image batch')\n",
    "parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n",
    "parser.add_argument('--conf-thres', type=float, default=0.001, help='object confidence threshold')\n",
    "parser.add_argument('--iou-thres', type=float, default=0.65, help='IOU threshold for NMS')\n",
    "parser.add_argument('--task', default='val', help='train, val, test, speed or study')\n",
    "parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "parser.add_argument('--single-cls', action='store_true', help='treat as single-class dataset')\n",
    "parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
    "parser.add_argument('--verbose', action='store_true', help='report mAP by class')\n",
    "parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
    "parser.add_argument('--save-hybrid', action='store_true', help='save label+prediction hybrid results to *.txt')\n",
    "parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
    "parser.add_argument('--save-json', action='store_true', help='save a cocoapi-compatible JSON results file')\n",
    "parser.add_argument('--project', default='runs/test', help='save to project/name')\n",
    "parser.add_argument('--name', default='exp', help='save to project/name')\n",
    "parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
    "parser.add_argument('--no-trace', action='store_true', help='don`t trace model')\n",
    "parser.add_argument('--v5-metric', action='store_true', help='assume maximum recall as 1.0 in AP calculation')\n",
    "# opt = parser.parse_args()]\n",
    "opt = parser.parse_args(args=[])\n",
    "# from collections import defaultdict\n",
    "# opt = defaultdict()\n",
    "# --data data/coco.yaml --img 640 --batch 32 --conf 0.001 --iou 0.65 --device 0 --weights yolov7.pt --name yolov7_640_val\n",
    "opt.img = 640\n",
    "opt.batch = 32\n",
    "opt.conf = 0.001\n",
    "opt.iou = 0.65\n",
    "opt.device = \"0\"\n",
    "opt.weights = \"yolo7.pt\"\n",
    "opt.name = \"yolov7_640_val\"\n",
    "\n",
    "opt.save_json |= opt.data.endswith('coco.yaml')\n",
    "opt.data = check_file(opt.data)  # check file\n",
    "print(opt)\n",
    "#check_requirements()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(opt.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR ðŸš€ 2023-4-29 torch 2.0.0+cu117 CUDA:0 (NVIDIA GeForce RTX 3090, 24268.3125MB)\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/sjjung/yolov7/utils/google_utils.py:26\u001b[0m, in \u001b[0;36mattempt_download\u001b[0;34m(file, repo)\u001b[0m\n\u001b[1;32m     25\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://api.github.com/repos/\u001b[39m\u001b[39m{\u001b[39;00mrepo\u001b[39m}\u001b[39;00m\u001b[39m/releases/latest\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mjson()  \u001b[39m# github api\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m assets \u001b[39m=\u001b[39m [x[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m response[\u001b[39m'\u001b[39;49m\u001b[39massets\u001b[39;49m\u001b[39m'\u001b[39;49m]]  \u001b[39m# release assets\u001b[39;00m\n\u001b[1;32m     27\u001b[0m tag \u001b[39m=\u001b[39m response[\u001b[39m'\u001b[39m\u001b[39mtag_name\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# i.e. 'v1.0'\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'assets'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m opt\u001b[39m.\u001b[39mtask \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# run normally\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m     test(opt\u001b[39m.\u001b[39;49mdata,\n\u001b[1;32m      3\u001b[0m         opt\u001b[39m.\u001b[39;49mweights,\n\u001b[1;32m      4\u001b[0m         opt\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[1;32m      5\u001b[0m         opt\u001b[39m.\u001b[39;49mimg_size,\n\u001b[1;32m      6\u001b[0m         opt\u001b[39m.\u001b[39;49mconf_thres,\n\u001b[1;32m      7\u001b[0m         opt\u001b[39m.\u001b[39;49miou_thres,\n\u001b[1;32m      8\u001b[0m         opt\u001b[39m.\u001b[39;49msave_json,\n\u001b[1;32m      9\u001b[0m         opt\u001b[39m.\u001b[39;49msingle_cls,\n\u001b[1;32m     10\u001b[0m         opt\u001b[39m.\u001b[39;49maugment,\n\u001b[1;32m     11\u001b[0m         opt\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m     12\u001b[0m         save_txt\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49msave_txt \u001b[39m|\u001b[39;49m opt\u001b[39m.\u001b[39;49msave_hybrid,\n\u001b[1;32m     13\u001b[0m         save_hybrid\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49msave_hybrid,\n\u001b[1;32m     14\u001b[0m         save_conf\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49msave_conf,\n\u001b[1;32m     15\u001b[0m         trace\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m opt\u001b[39m.\u001b[39;49mno_trace,\n\u001b[1;32m     16\u001b[0m         v5_metric\u001b[39m=\u001b[39;49mopt\u001b[39m.\u001b[39;49mv5_metric\n\u001b[1;32m     17\u001b[0m         )\n\u001b[1;32m     19\u001b[0m \u001b[39melif\u001b[39;00m opt\u001b[39m.\u001b[39mtask \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mspeed\u001b[39m\u001b[39m'\u001b[39m:  \u001b[39m# speed benchmarks\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m opt\u001b[39m.\u001b[39mweights:\n",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(data, weights, batch_size, imgsz, conf_thres, iou_thres, save_json, single_cls, augment, verbose, model, dataloader, save_dir, save_txt, save_hybrid, save_conf, plots, wandb_logger, compute_loss, half_precision, trace, is_coco, v5_metric)\u001b[0m\n\u001b[1;32m     35\u001b[0m (save_dir \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m save_txt \u001b[39melse\u001b[39;00m save_dir)\u001b[39m.\u001b[39mmkdir(parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# make dir\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m# Load model\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m model \u001b[39m=\u001b[39m attempt_load(weights, map_location\u001b[39m=\u001b[39;49mdevice)  \u001b[39m# load FP32 model\u001b[39;00m\n\u001b[1;32m     39\u001b[0m gs \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mint\u001b[39m(model\u001b[39m.\u001b[39mstride\u001b[39m.\u001b[39mmax()), \u001b[39m32\u001b[39m)  \u001b[39m# grid size (max stride)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m imgsz \u001b[39m=\u001b[39m check_img_size(imgsz, s\u001b[39m=\u001b[39mgs)  \u001b[39m# check img_size\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/sjjung/yolov7/models/experimental.py:251\u001b[0m, in \u001b[0;36mattempt_load\u001b[0;34m(weights, map_location)\u001b[0m\n\u001b[1;32m    249\u001b[0m model \u001b[39m=\u001b[39m Ensemble()\n\u001b[1;32m    250\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m weights \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(weights, \u001b[39mlist\u001b[39m) \u001b[39melse\u001b[39;00m [weights]:\n\u001b[0;32m--> 251\u001b[0m     attempt_download(w)\n\u001b[1;32m    252\u001b[0m     ckpt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(w, map_location\u001b[39m=\u001b[39mmap_location)  \u001b[39m# load\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     model\u001b[39m.\u001b[39mappend(ckpt[\u001b[39m'\u001b[39m\u001b[39mema\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m ckpt\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mema\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mfuse()\u001b[39m.\u001b[39meval())  \u001b[39m# FP32 model\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/sjjung/yolov7/utils/google_utils.py:31\u001b[0m, in \u001b[0;36mattempt_download\u001b[0;34m(file, repo)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mexcept\u001b[39;00m:  \u001b[39m# fallback plan\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     assets \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39myolov7.pt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39myolov7-tiny.pt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39myolov7x.pt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39myolov7-d6.pt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39myolov7-e6.pt\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     30\u001b[0m               \u001b[39m'\u001b[39m\u001b[39myolov7-e6e.pt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39myolov7-w6.pt\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 31\u001b[0m     tag \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mcheck_output(\u001b[39m'\u001b[39;49m\u001b[39mgit tag\u001b[39;49m\u001b[39m'\u001b[39;49m, shell\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mdecode()\u001b[39m.\u001b[39;49msplit()[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[1;32m     33\u001b[0m name \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mname\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m assets:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "if opt.task in ('train', 'val', 'test'):  # run normally\n",
    "    test(opt.data,\n",
    "        opt.weights,\n",
    "        opt.batch_size,\n",
    "        opt.img_size,\n",
    "        opt.conf_thres,\n",
    "        opt.iou_thres,\n",
    "        opt.save_json,\n",
    "        opt.single_cls,\n",
    "        opt.augment,\n",
    "        opt.verbose,\n",
    "        save_txt=opt.save_txt | opt.save_hybrid,\n",
    "        save_hybrid=opt.save_hybrid,\n",
    "        save_conf=opt.save_conf,\n",
    "        trace=not opt.no_trace,\n",
    "        v5_metric=opt.v5_metric\n",
    "        )\n",
    "\n",
    "elif opt.task == 'speed':  # speed benchmarks\n",
    "    for w in opt.weights:\n",
    "        test(opt.data, w, opt.batch_size, opt.img_size, 0.25, 0.45, save_json=False, plots=False, v5_metric=opt.v5_metric)\n",
    "\n",
    "elif opt.task == 'study':  # run over a range of settings and save/plot\n",
    "    # python test.py --task study --data coco.yaml --iou 0.65 --weights yolov7.pt\n",
    "    x = list(range(256, 1536 + 128, 128))  # x axis (image sizes)\n",
    "    for w in opt.weights:\n",
    "        f = f'study_{Path(opt.data).stem}_{Path(w).stem}.txt'  # filename to save to\n",
    "        y = []  # y axis\n",
    "        for i in x:  # img-size\n",
    "            print(f'\\nRunning {f} point {i}...')\n",
    "            r, _, t = test(opt.data, w, opt.batch_size, i, opt.conf_thres, opt.iou_thres, opt.save_json,\n",
    "                            plots=False, v5_metric=opt.v5_metric)\n",
    "            y.append(r + t)  # results and times\n",
    "        np.savetxt(f, y, fmt='%10.4g')  # save\n",
    "    os.system('zip -r study.zip study_*.txt')\n",
    "    plot_study_txt(x=x)  # plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
